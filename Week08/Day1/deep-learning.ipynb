{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Network\n",
    "We are now going to practice creating our first network. We will be using Docker container we have setup in the previous activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "We are going to work with an example from the article Writing your first Neural Net in less than 30 lines of code with Keras. We made small edits because the article is not using a Docker environment.\n",
    "\n",
    "As usually, we start with the import of the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` has several datasets we can use to learn and, luckily for us, `MNIST` is among them! If you are not familiar with this dataset you can read more about it in the Wikipedia article MNIST database.\n",
    "\n",
    "`Models` and `Layers` are both modules that will help us build our NN (that is all you need to know for now :) ) and `to_categorical` is used for our data encoding â€“ but more on that later!\n",
    "\n",
    "Now that we have the required modules imported, we will want to split our dataset into train and test sets. This can be simply accomplished with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't actually split the data in Python but we have loaded it to train and test samples directly from mnist.\n",
    "\n",
    "Next, we can check the shape of our data to understand it better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our train sample, we can see that we have 60,000 images with the size 28x28 pixels. We have 60,000 labels labeling the images from 0 to 9.\n",
    "\n",
    "We are now going to build our first network which will predict what number is in the picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(784, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(784, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code above, we've created our first `network`. We did three layers but we will talk more about this during the following week. The goal of this first tutorial is to show us that creating a network doesn't necessarily have be too complicated and it can take only a couple of rows of code.\n",
    "\n",
    "Before we can feed our data into our newly created model, we will need to reshape our input into a format that the model can read. The original shape of our input was [60000, 28, 28] which essentially represents 60,000 images with the pixel height and width of 28x28. We will reshape it, so that we have all pixels for each image in one row of a 2D array. We can think about this as a dataset with 60,000 rows and 28*28 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to make sure our network thinks it is a categorical problem because numbers from 0 to 9 can be interpreted as regression as well. So we will encode our target as categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = (to_categorical(train_labels))\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = sample(list(train_labels), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels = sample(list(test_labels), 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our dataset split into training and test sets, with our model compiled, and with our data reshaped and encoded, we are now ready to train our NN! To do this, we will call the fit function and pass in the required parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60/60 [==============================] - 2s 25ms/step - loss: 0.0549 - accuracy: 0.9839\n",
      "Epoch 2/5\n",
      "60/60 [==============================] - 1s 25ms/step - loss: 0.0401 - accuracy: 0.9887\n",
      "Epoch 3/5\n",
      "60/60 [==============================] - 1s 24ms/step - loss: 0.0274 - accuracy: 0.9927\n",
      "Epoch 4/5\n",
      "60/60 [==============================] - 1s 24ms/step - loss: 0.0208 - accuracy: 0.9942\n",
      "Epoch 5/5\n",
      "60/60 [==============================] - 1s 24ms/step - loss: 0.0152 - accuracy: 0.9961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e125232340>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass in `epochs`, which dictate the number of backward and forward propagations (more about this during the upcoming week as well), and the batch_size, which indicates the number of training samples per backward/forward propagation.\n",
    "\n",
    "Now, we can check the performance of our network on test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0847 - accuracy: 0.9732\n",
      "test_acc: 0.9732000231742859 test_loss 0.08467981219291687\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc, 'test_loss', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should get an accuracy of around 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We have just taken the first step on our deep learning journey. We have seen that creating a network and using it as a black box is not all that complex. However, in order to maximize the added value of using deep learning networks, it's fundamental to also understand what is going on during the different steps."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "371c638ddd7450534f6448490a06a9a2d0ce6381e2d95922f69b49070e41ac13"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
